{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近在看《集体智慧编程这本书》，再次学习k-means算法，虽然之前接触过很多，但是没有完全自己实现过，这次就试着根据算法思想，手写k-means算法。\n",
    "\n",
    "这里使用的数据是书里面的博客数据，第一行是表头，第一列是博客名称，后面的列是每个单词出现的次数，大概长这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blog\tbe\tnot\tyour\n",
    "Signal v. Noise - Medium\t46\t32\t47\n",
    "Eschaton\t13\t21\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means算法的思想大概是这样子的：\n",
    "\n",
    "- 先随机初始化k个点，称为中心点\n",
    "- 对于数据集中的每条记录，计算它与每个中心点的距离，把它归到距离它最近的中心点的分组中\n",
    "- 对于每个分组，通过均值计算中心点，得到新的中心点\n",
    "- 重复这个过程，直到每条记录的分组不再变化为止\n",
    "\n",
    "对于距离的计算，我使用皮尔逊相似度来计算，下面是距离计算的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(vec):\n",
    "    values = [value for value in vec]\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "\n",
    "def distance_pearson(left, right):\n",
    "    # 计算均值\n",
    "    left_mean = get_mean(left)\n",
    "    right_mean = get_mean(right)\n",
    "    # 协方差\n",
    "    covariant = sum([(left[key] - left_mean) * (right[key] - right_mean) for key in range(len(left))])\n",
    "    # 标准差\n",
    "    left_standard = math.sqrt(sum([math.pow(left[key] - left_mean, 2) for key in range(len(left))]))\n",
    "    right_standard = math.sqrt(sum([math.pow(right[key] - right_mean, 2) for key in range(len(left))]))\n",
    "\n",
    "    # 1 - 皮尔逊相关度，越接近0距离越近，反之越远\n",
    "    return 1 - (covariant / (left_standard * right_standard))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是k-means的实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(rows, distance=distance_pearson, k=4):\n",
    "    # 共m条记录，n个特征, 也就是n个单词\n",
    "    rows = np.array(rows)\n",
    "    m, n = np.shape(rows)\n",
    "    # 求出每个特征的最小值和最大值\n",
    "    mins = rows.min(axis=0)\n",
    "    maxs = rows.max(axis=0)\n",
    "\n",
    "    # 初始化中心点\n",
    "    centers = []\n",
    "    clusters = [[] for i in range(k)]\n",
    "    for i in range(k):\n",
    "        # 随机生成的中心点，每个特征必须在最小值和最大值范围内\n",
    "        center = [random.randrange(mins[i], maxs[i]) for i in range(n)]\n",
    "        centers.append(center)\n",
    "    # 一直进行聚类，只有分组不再变化时才停止\n",
    "    change = True\n",
    "    while change:\n",
    "        change = False\n",
    "        # 对于每条记录，都计算距离\n",
    "        for i in range(m):\n",
    "            closest = 0xffff\n",
    "            closest_center = 0\n",
    "            # 找到距离最近的中心点\n",
    "            for j in range(len(centers)):\n",
    "                d = distance(rows[i], centers[j])\n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    closest_center = j\n",
    "            # 新该记录添加到该分组中，如果已经在了，就不用了\n",
    "            if i not in clusters[closest_center]:\n",
    "                clusters[closest_center].append(i)\n",
    "                change = True\n",
    "        # 通过均值重新计算分组的中心点\n",
    "        for i in range(len(clusters)):\n",
    "            cluster_rows = [rows[cluster] for cluster in clusters[i]]\n",
    "            centers[i] = np.mean(cluster_rows, axis=0)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来对博客的数据进行聚类:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names, row_names, rows = read_file('./blogdata.txt')\n",
    "clusters = k_means(rows, k=4)\n",
    "for i in range(len(clusters)):\n",
    "    for j in clusters[i]:\n",
    "        print(row_names[j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "看一下效果，是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eschaton\n",
    "Guy Kawasaki\n",
    "Google Blogoscoped\n",
    "The Dish\n",
    "The Write News\n",
    "blog maverick\n",
    "Boing Boing\n",
    "BuzzMachine\n",
    "Copyblogger\n",
    "Latest from Crooks and Liars\n",
    "Deadspin\n",
    "kottke.org\n",
    "Lifehack\n",
    "TechCrunch\n",
    "TMZ.com\n",
    "Seth Godin's Blog on marketing, tribes and respect\n",
    "Captain's Quarters\n",
    "Joel on Software\n",
    "Kotaku\n",
    "ShoeMoney\n",
    "SpikedHumor - Today's Videos and Pictures\n",
    "Signal v. Noise - Medium\n",
    "\n",
    "John Battelle's Search Blog\n",
    "Creating Passionate Users\n",
    "Joi Ito's Web\n",
    "The Viral Garden\n",
    "PaulStamatiou.com - Technology, Design and Photography\n",
    "WIL WHEATON dot NET\n",
    "43 Folders\n",
    "456 Berea Street\n",
    "Joel on Software\n",
    "Kotaku\n",
    "Lifehacker\n",
    "Matt Cutts: Gadgets, Google, and SEO\n",
    "mezzoblue\n",
    "Neil Gaiman's Journal\n",
    "plasticbag.org\n",
    "Derek Powazek\n",
    "ProBlogger\n",
    "SpikedHumor - Today's Videos and Pictures\n",
    "Steve Pavlina\n",
    "ongoing by Tim Bray\n",
    "BuzzMachine\n",
    "Copyblogger\n",
    "Guy Kawasaki\n",
    "\n",
    "Google Operating System\n",
    "NewsBusters\n",
    "O'Reilly Radar\n",
    "Seth Godin's Blog on marketing, tribes and respect\n",
    "Slashdot\n",
    "ThinkProgress\n",
    "Engadget RSS Feed\n",
    "Gothamist\n",
    "PerezHilton\n",
    "Quick Online Tips\n",
    "Search Engine Roundtable\n",
    "ShoeMoney\n",
    "Techdirt.\n",
    "Wired\n",
    "The Write News\n",
    "Schneier on Security\n",
    "TechCrunch\n",
    "Captain's Quarters\n",
    "\n",
    "Signal v. Noise - Medium\n",
    "Celebslam\n",
    "The Official Google Blog\n",
    "Mashable\n",
    "Autoblog\n",
    "Captain's Quarters\n",
    "Schneier on Security\n",
    "Google Blogoscoped\n",
    "Lifehacker\n",
    "Quick Online Tips\n",
    "Google Operating System\n",
    "Joel on Software\n",
    "Search Engine Roundtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到已经成功聚成了4类，表面看，好像看不出来聚类的效果怎么样，就当作拿来练手，实现的过程中，也加深了对这个算法的理解。"
   ]
  }
 ],
 "metadata": {
  "front-matter": {
   "date": "2018-05-20",
   "slug": "implement-k-means-algorithm-myself",
   "subtitle": "Generic subtitle",
   "tags": [
    "机器学习"
   ],
   "title": "Implement k-means algorithm myself"
  },
  "hugo-jupyter": {
   "render-to": "content/post/"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
